[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/How does significance level affect Type I and Type II Errors/index.html",
    "href": "posts/How does significance level affect Type I and Type II Errors/index.html",
    "title": "How does significan level affect Type I and Type II errors",
    "section": "",
    "text": "It is always the most important thing to set up a significance level once we start doing statistical analysis. The logic of setting up actually comes from the idea of the tradeoff considering Type I and Type II errors. To illustrate the idea, we will introduce ideas about Type I and Type II errors, and then simulate a one-sample test in an example in python."
  },
  {
    "objectID": "posts/How does significance level affect Type I and Type II Errors/index.html#how-to-balance-type-i-and-type-ii-error",
    "href": "posts/How does significance level affect Type I and Type II Errors/index.html#how-to-balance-type-i-and-type-ii-error",
    "title": "How does significan level affect Type I and Type II errors",
    "section": "How to balance Type I and Type II error?",
    "text": "How to balance Type I and Type II error?\nHere comes a question: How does the setting of alpha affects the two errors? In the following part, we tried several numbers of alpha to test the type I and type II errors. In this example, we create a population of 1000 elements with a mean of 100 and a standard deviation of 15.\n\n\nCode\nimport numpy as np\n\nimport pandas as pd\n\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\n\nimport math\n\nimport random \n\nimport seaborn as sns\n\nsns.set(color_codes=True)\n\n# create a population of 1000 elements in normal distribution. Mean =100, Std dev = 15.\n\npop = np.random.normal(100, 15, 1000)\n\npop.dtype\n\nsns.displot(pop)\n\n\n\n\n\nThen, we obtain sample data from this population.The Sample size is 30.\n\n\nCode\nsamples = np.random.choice(pop,30,replace=True)\n\nplt.figure(\"Test Samples\")\n\nsns.displot(samples, label='Sample') \n\nplt.legend()\n\nplt.show()\n\nprint (\"Sample Summary\")\n\nstats.describe(samples)\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\nSample Summary\n\n\nDescribeResult(nobs=30, minmax=(61.54451654708927, 142.92816483483105), mean=101.6988644538074, variance=241.70004237437374, skewness=0.41107555643476634, kurtosis=1.5583003284030008)\n\n\nIn 1000 simulations, the counts of Type I and Type II errors were recorded for different alpha values.We calculated and printed the Type I error rates and Type II error rates shown as below.\n\n\nCode\nimport numpy as np\n\nfrom scipy.stats import ttest_1samp\n\n# Parameters\n\npopulation_mean = 100  # True population mean\n\npopulation_stddev = 15  # Population standard deviation\n\nsample_size = 30  # Size of each sample\n\nnum_simulations = 1000  # Number of simulations\n\n# Different alpha values to test\n\nalpha_values = [0.01, 0.05, 0.10]  # Significance levels (1%, 5%, 10%)\n\n# Perform simulations for different alpha values\n\nfor alpha in alpha_values:\n\n    type_i_errors = 0  # Counter for Type I errors\n\n    type_ii_errors = 0  # Counter for Type II errors\n\n    \n\n    for _ in range(num_simulations):\n\n        # Generate a random sample from the population\n\n        sample = np.random.normal(loc=population_mean, scale=population_stddev, size=sample_size)\n\n        \n\n        # Perform a one-sample t-test\n\n        t_stat, p_value = ttest_1samp(sample, popmean=population_mean)\n\n        \n\n        # Check for Type I error (reject null hypothesis when it is true)\n\n        if p_value &lt; alpha:\n\n            type_i_errors += 1\n\n        else:\n\n            # Check for Type II error (fail to reject null hypothesis when it is false)\n\n            type_ii_errors += 1\n\n    \n\n    # Calculate error rates for the current alpha value\n\n    type_i_error_rate = type_i_errors / num_simulations\n\n    type_ii_error_rate = type_ii_errors / num_simulations\n\n    \n\n    # Print results\n\n    print(f\"Alpha: {alpha}\")\n\n    print(f\"Type I Error Rate: {type_i_error_rate:.4f}\")\n\n    print(f\"Type II Error Rate: {type_ii_error_rate:.4f}\")\n\n    print(\"------\")\n\n\nAlpha: 0.01\nType I Error Rate: 0.0150\nType II Error Rate: 0.9850\n------\nAlpha: 0.05\nType I Error Rate: 0.0490\nType II Error Rate: 0.9510\n------\nAlpha: 0.1\nType I Error Rate: 0.1020\nType II Error Rate: 0.8980\n------\n\n\nThe results clearly shows that as value of alpha is increases from 0.01 to 0.1, the probability of type I errors also increases.However, as alpha increases, the probability of type II errors decreases. By increasing alpha, the number of false positives increases, but the number of false negatives decreases.\nThe results reveal an idea: there is always a trade off between false positives and false negatives.Within the concept of “significance,” there is embedded a trade-off between these two types of errors.\nGenerally, the value of \\(\\alpha\\) is considered a reasonable compromise between these two types of errors, while several debates exists."
  },
  {
    "objectID": "posts/How does significance level affect Type I and Type II Errors/index.html#why-alpha-as-0.05-is-prevelant-in-statistical-analysis",
    "href": "posts/How does significance level affect Type I and Type II Errors/index.html#why-alpha-as-0.05-is-prevelant-in-statistical-analysis",
    "title": "How does significan level affect Type I and Type II errors",
    "section": "Why alpha as 0.05 is prevelant in statistical analysis?",
    "text": "Why alpha as 0.05 is prevelant in statistical analysis?\nThe cutoff for statistical significance at 0.05 is essentially arbitrarily used in many fields, largely because Ronald Fisher proposed it in his massively influential book, Statistical Methods for Research Workers.\nFisher, who can legitimately be said to be the father of modern parametric statistical analysis, proposed that a cut-off of 0.05, which would mean that a true null hypothesis would be incorrectly rejected (i.e. a false positive) 5% of the time, was a reasonable rate of error, that would be overcome by the preponderance of failures to reject any true null hypotheses. The logic behind 0.05 instead of other numbers comes from the 2-sigma interval (\\(2 \\sigma\\), which is two times of standard deviation) of a normal distribution covers 95% of the data. The normal distribution effectively describes the statistical distribution of many complex phenomena in nature.\nHowever, it’s far from universally accepted. In many fields where large amounts of data are being analysed, it is standard to use a cut-off rate of 0.01, or even 0.001. In analysing fMRI data, for example, it is common to use 0.001 as a cut-off, in addition to using e.g. cluster correction, so as to reduce the rate of false positives.\nIn fact, not only is 0.05 not really a magic number, p-values are losing their prominence as increased computing power and accessibie programming languages such as R which make more advanced analysis methods available to a broader audience. For example, the Bayesian statistics shifts the likelihood of data under a specific hypothesis to evaluating the probability of a hypothesis given the observed data. It allows for the estimation of posterior likelihood distributions, providing a more comprehensive and intuitive understanding of the data."
  },
  {
    "objectID": "posts/How does significance level affect Type I and Type II Errors/index.html#reference",
    "href": "posts/How does significance level affect Type I and Type II Errors/index.html#reference",
    "title": "How does significan level affect Type I and Type II errors",
    "section": "Reference",
    "text": "Reference\n1. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6532382/\n2. https://www.scribbr.com/statistics/type-i-and-type-ii-errors/\n3. https://online.stat.psu.edu/stat200/lesson/6/6.1\n4. https://github.com/learn-co-curriculum/dsc-type-1-and-2-error-lab/blob/master/index.ipynb"
  },
  {
    "objectID": "posts/Everything about James-Stein Estimator/index.html",
    "href": "posts/Everything about James-Stein Estimator/index.html",
    "title": "Everything about James-Stein Estimator",
    "section": "",
    "text": "What is the James-Stein Estimator?\nNamed by statisticians William James and Charles Stein, the James-Stein estimator(JS estimator) is a statistical estimator that is used for simultaneously estimating multiple means. In 1961, the two statisticians showed that, in certain situations, when estimating the means of normally distributed populations, an estimator that “shrinks” the observed sample means towards each other or towards an overall mean can be expected to be more accurate (in terms of mean squared error) than the traditional sample mean estimator.\nThe setting of JS estimator are listed as below:\n\nWe have x1, x2, …, xm measurements for the true values\n\\({\\theta}_1,{\\theta}_2,...,{\\theta}_m\\) with average \\(\\bar{x} = \\sum_{i=1}^{m}x_i/m\\) and standard deviation \\({\\sigma}= \\sqrt{\\sum_{i=1}^m(x_i-x)^2/(m-1)}\\)\nThe measurement error standard deviation is known to be \\(\\sigma_x\\)\nThe the estimate(JS estimator) of the true value is given by\n\\[\n\\hat{\\theta}_i = \\left( 1 - \\frac{(m - 3) \\sigma_{x}^2}{(m - 1) \\sigma^2} \\right)x_i + \\frac{(m - 3) \\sigma_{x}^2}{(m - 1) \\sigma^2}\\bar{x}\n\\]\nNote that the factor that multiplies \\(x_i\\) could turn out to be negative. The positive part Jame-Stein estimator is even better and is given by\n\\[\n\\hat{\\theta}_{i+} = \\left( 1 - \\frac{(m - 3)\\sigma_{x}^2}{(m - 1)\\sigma^2} \\right)^{+} (x_i - \\bar{x}) + \\bar{x}\n\\]\nwhere \\((1-\\frac{(m-3)\\sigma_x^2}{(m-1)\\sigma^2})^+=(1-\\frac{(m-3)\\sigma_x^2}{(m-1)\\sigma^2})\\) if the term is positive and 0 if it is negative.\n\nIt is common in regression analysis to take averages to make predictions. For example, the sample mean( the average score from all samples) is used as an estimator for the population mean. JS estimator, known for its counter-intuitive property of “shrinkage”, improves upon these average by shrinking them towards a more central average.\n\n\nMotivation behind JS Estimator\nThe James-Stein estimator is used in the context of statistical estimation, particularly for estimating the means of multiple normal distributions when the variances are known and equal. It is an example of shrinkage estimation, where estimates are “shrunk” towards a central value (usually the overall mean) to improve accuracy. To explain the meaning of JS estimator, we take an example of a pavement assessment in the next part.\n\n\nAn Example of Friction Assessment\n\nFrictions are important properties of pavement when it comes to crashes. Friction generally expresses between 0 and 100 with 0 representing complete lack of friction, and 100 being maximum friction. Suppose we have a device that measures the friction and any measurement performed consists of the real friction value with a random error having a standard deviation of 3. Two testers, Abby and Betty, are taking separate measurement of frictions applying this device. Abby takes one measurement on each of 100 unrelated pavement sections. Betty, on the other hand, takes 100 measurements on the same section. Assume that the data got by Betty, distributes with a standard deviation in 3. Then different standard deviation of Abby’s data might cause different understanding during the regression analysis.\n\nIf Abby got the standard deviation in 3, she might suspect that the standard deviation reflect the standard deviation of the measurement error. We would assume all 100 sections have the same or close measurements to get a better estimate of the friction on each section.\nIf Abby got standard deviation of 30, she might assume there is a lot of variability in the friction values of the 100 sections and the sections have distinct friction values. We should not average the sections together as they are distinct.\n\nSo here comes a question, what if Abby got a series of data with a standard deviation of 6? It is larger than measurement error standard deviation(3 in Betty’s data) but not very large. If Abby average, she will reduce the measurement error but also average the true different values which will increase errors of estimated friction.\nWhen we are faced with multiple estimation problems and each problem has its own measurement error, the JS estimator provides a way to optimize a single estimate by integrating all the estimates. In this example, When the standard deviation of the measurement errors is small (e.g., when Abby’s standard deviation of measurement errors is similar to Betty’s standard deviation of measurement errors), this suggests that the measurements from the individual road segments are relatively reliable, and the James-Stein estimator will recommend that we give more weight to these individual measurements. Conversely, when the standard deviation of Abby’s measurements is greater than the standard deviation of the measurement error, for example much greater than 3, this implies that there is a large amount of true variability between the different road segments, and the James-Stein estimator would then recommend that we give relatively little weight to these individual measurements in the estimation, and rely more on the overall mean.\n\nIn the example, if Abby’s data has a standard deviation of 6, this suggests that there is variability above and beyond the measurement error alone, but that this variability is not as extreme as a standard deviation of 30. In this case, the James-Stein estimator will find a point in bewteen and estimate the true friction value for each section by taking a weighted average of the individual measurements and the overall mean. The exact weighting ratio is adjusted according to the results of comparing the standard deviation of the data with the known standard deviation of the measurement error.\nTo illustrate the idea, we will show an example of how we can implement the positive part JS estimator and show that it improves the mean square error between the estimated value and the true value compared to the maximum likelihood estimate. We will assume that the true friction measurements on the 100 sections have mean 50 and standard deviation of 10. For the device we are using to measure the friction, the standard deviation of the measurement is know and we will look at three cases where the standard deviation is 2, 10, and 30. We will compared the mean square error of the JS estimator and the maximum likelihood estimator.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to calculate James-Stein Estimator\ndef james_stein(x, error_std):\n    n = len(x)\n    variance = error_std**2\n    theta_hat = ((n - 3) * variance / np.sum((x - np.mean(x))**2)) * x + ((1 - (n - 3) * variance / np.sum((x - np.mean(x))**2)) * np.mean(x))\n    return theta_hat\n\n# Generating normally distributed true friction values\ntheta = 50 + 10 * np.random.randn(100)\nplt.hist(theta, bins='auto')  # 'bins' can be set to an integer for a specific number of bins or 'auto' for automatic binning\nplt.title('Histogram of Theta')\nplt.xlabel('Theta values')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n# Measurements with different measurement error standard deviations\nx2 = theta + 2 * np.random.randn(100)\nx10 = theta + 10 * np.random.randn(100)\nx20 = theta + 20 * np.random.randn(100)\n\n# Calculating James-Stein estimator for each case\ntheta_hat2 = james_stein(x2, 2)\ntheta_hat10 = james_stein(x10, 10)\ntheta_hat20 = james_stein(x20, 20)\n\n# Calculating MSE\nMSE = np.array([\n    [np.mean((theta - x2)**2), np.mean((theta - theta_hat2)**2)],\n    [np.mean((theta - x10)**2), np.mean((theta - theta_hat10)**2)],\n    [np.mean((theta - x20)**2), np.mean((theta - theta_hat20)**2)]\n])\n\nprint(\"MSE:\")\nprint(MSE)\n\nMSE:\n[[  4.07847652 110.69315578]\n [121.19202847  56.00276867]\n [504.79429861 238.55737768]]\n\n\n\n# Plotting\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(x2, x2, label='Maximum Likelihood')\nplt.plot(x2, theta_hat2, label='James Stein')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(x10, x10, label='Maximum Likelihood')\nplt.plot(x10, theta_hat10, label='James Stein')\nplt.legend()\n\nplt.subplot(1, 3, 3)\nplt.plot(x20, x20, label='Maximum Likelihood')\nplt.plot(x20, theta_hat20, label='James Stein')\nplt.legend()\n\nplt.show()\n\n\n\n\nThe figure shows how the the James-Stein estimator shrinks the measurement towards the average of the measurements (here 50). For the case where the measurement error is very small, the measurements are almost unchanged by the JS estimator. As the measurement error increases, more weight is placed on the average and the amount of shrinkage is more pronounced."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "xiaoyingyang96.github.io",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nXiaoying Yang\n\n\n\n\n\n\n  \n\n\n\n\nClustering Penguins with KMeans: An Insightful Approach\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nXiaoying Yang\n\n\n\n\n\n\n  \n\n\n\n\nClassifying Fetal Health Conditions Using Machine Learning Algorithms and Cardiotocography (CTG) Data\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nXiaoying Yang\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Humidity Using Linear and Non-linear Regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nXiaoying Yang\n\n\n\n\n\n\n  \n\n\n\n\nEverything about James-Stein Estimator\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nXiaoying Yang\n\n\n\n\n\n\n  \n\n\n\n\nHow does significan level affect Type I and Type II errors\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nXiaoying Yang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Xiaoying Yang",
    "section": "",
    "text": "This blog was created for the final project of my machine learning class in Fall 2023 and will be continuously updated.\nAs a PhD student at Virginia Tech, I am interested in researching Brain-Computer Interfaces (BCI) and Augmented Reality (AR) in engineering fields. In my daily life, I enjoy playing the piano and violin, as well as various ball games! Living the questions is my joy. :)"
  },
  {
    "objectID": "posts/clustering penguins/index.html",
    "href": "posts/clustering penguins/index.html",
    "title": "Clustering Penguins with KMeans: An Insightful Approach",
    "section": "",
    "text": "Facts about Penguin Species\nPenguins are a family of 17 to 19 species of birds that live primarily in the Southern Hemisphere. They include the tiny blue penguins of Australia and New Zealand, the majestic emperor penguins of Antarctica and king penguins found on many sub- Antarctic islands, the endangered African penguin and the Galápagos penguin—the only penguin to be found north of the equator. Identifying penguin species can be both a fun and educational experience. Size, shape, plumage, color, beak size, beak color, and behavioral traits etc are key factors to consider when trying to differentiate between various penguin species. Therefore, identify penguins can be a difficult task.\n\n\nIntro of Clustering and KMeans\nClustering, an unsupervised learning method, groups samples in a dataset into various clusters based on similarity criteria. This method focuses on the intrinsic characteristics of the data itself, rather than relying on pre-labeled categories. The key advantage of clustering lies in its ability to uncover meaningful patterns and the intrinsic structure within the data, all without the need for explicit instructions. In this blog post, we demonstrate the process of applying KMeans clustering by classifying penguins species, providing a practical illustration of how KMeans can effectively discern and group data based on its inherent traits.\nKMeans is a distance-based clustering algorithm that treats data points that are relatively close together as similar and groups them together. The process of KMeans is as follows:\n(1) Randomly find k points as the center of mass (seed);\n(2) Calculate the distance from other points to the k seeds, and choose the closest one as the category of the point;\n(3) Update the center of mass of each category, iterate until the center of mass is unchanged.\n\n\nStep 0: Load & Preprocess data\n\nimport numpy as np\nimport pandas as pd\nimport os\nos.environ['OMP_NUM_THREADS'] = '2'\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n## Step 1: Load and preprocess data\n# load data, preprocess data where dealing with missing values, outliers\ndata = pd.read_csv('https://raw.githubusercontent.com/xiaoyingyang96/xiaoyingyang96.github.io/main/Data/penguins.csv')\n#data = pd.read_csv('D:\\\\Courses\\\\2023 Fall\\\\CS 5805 Machine Learning\\\\clustering\\\\penguins.csv')\n#Missing values\nprint(data.sex.unique())\n\n['MALE' 'FEMALE' nan '.']\n\n\nIn terms of the result above, the sex attribute contains missing and erroneous data, it is necessary to remove them. And in the outlier detection, let us consider the boxplot diagram. This diagram shows the statistics of the sampled data, the distribution of elements.\n\ndata[data.sex == '.']\ndata = data.drop(index = 336)\ndata.isnull().sum()\ndata = data.dropna()\ndata.boxplot()\nplt.show()\n\n\n\n\nFrom the boxplot it is easy to detect that there are outliers (circles) in the column flipper_length_mm (flipper length). Thus, we need to remove these data so that they do not affect the performance of the model.\n\ndata[data[\"flipper_length_mm\"] &gt; 4000]\ndata[data[\"flipper_length_mm\"] &lt; 0]\npenguins_clean = data.drop([9, 14])\ndf = pd.get_dummies(data)\n\n\n\nStep 1: Feature Scaling & Transformation\nWe will perform feature scaling and data dimensionality reduction using PCA (Principal Component Analysis). Data dimensionality reduction is the process of reducing the number of features (variables) in a dataset while preserving the most important characteristics of the information. PCA extracts the most important principal components from multidimensional data by projecting them onto a new feature space. Each principal component is a combination of the original features that describes the greatest amount of variability in the data. The scaled feature value are shown in the following table.\n\nscaler = StandardScaler()\nX = scaler.fit_transform(df)\npenguins_preprocessed = pd.DataFrame(data=X, columns=df.columns)\npenguins_preprocessed.head(10)\n\n#PCA Transformation \npca = PCA(n_components=2)\npenguins_PCA = pca.fit_transform(penguins_preprocessed)\n\n\n\nStep 2: Choose the Number of Clusters(K) by Elbow method\nAn appropriate K value is a key to conduct KMeans for clustering. The elbow method provides a way to balance the number of clusters and the quality of clusters.\nIn the context of K-means clustering, the Elbow Method graph serves as a specific type of model performance curve, depicting how inertia values change with different numbers of clusters (K values). Inertia, defined as the sum of the distances between data points and their nearest cluster center, is a key metric for assessing the quality of clustering. Lower inertia values indicate that data points are closer to their cluster centers, typically suggesting better clustering outcomes.\nIn K-means clustering, as the number of clusters increases, each data point is more likely to be closer to its cluster center, thereby reducing the inertia. However, beyond a certain point, adding more clusters does not significantly improve the quality of clustering, and the rate of decrease in inertia slows down. This slowing point, akin to the bending of an elbow, is the namesake of the Elbow Method. It marks the balance point between the number of clusters and the quality of clustering, hence is often considered the criterion for selecting the optimal number of clusters.\nApplying elbow method in the model performance curve, we pick K = 4 as appropriate number of clusters.\n\ninertia = []\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(penguins_PCA)\n    inertia.append(kmeans.inertia_)\nplt.clf()\nplt.plot(range(1, 10), inertia, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Method\")\nplt.show()\n\n\n\n\n\n\nStep 3: Create and Fit the KMeans Model\nInstantiate the KMeans class and fit to the data. Then, we retrieve the cluster centroids (the mean of the points in each cluster) and the labels (which point belongs to which cluster).\n\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(penguins_PCA)\nplt.clf()\nplt.scatter(penguins_PCA[:, 0], penguins_PCA[:, 1], c=kmeans.labels_, cmap=\"viridis\")\nplt.xlabel(\"First Principal Component\")\nplt.ylabel(\"Second Principal Component\")\nplt.title(f\"K-means Clustering (K={n_clusters})\")\nplt.show()\n\n\n\n\nIn the clustering diagram, the four clusters are distinctly separated, each denoted by a unique color, indicating the K-means algorithm’s effectiveness in grouping data points according to their mutual similarities. The purple cluster in the bottom left and the yellow cluster in the top right appear more dense, suggesting a higher degree of similarity among their respective data points. In contrast, the green cluster on the middle right and the blue cluster on the middle left are more dispersed, hinting at greater variation within those clusters. There are no apparent outliers, as no data points are significantly distant from their cluster’s core, implying a relatively uniform data distribution. The demarcations between clusters, particularly between the purple and yellow ones, are quite pronounced. However, the boundary between the blue and green clusters is somewhat blurred, which may reflect a degree of overlap in the characteristics of these clusters’ data points.\n\n\nStep 4: Evaluate the clustering performance\n\n#Calculation of the silhouette coefficient\nsilhouette_avg = silhouette_score(penguins_PCA, kmeans.labels_)\n\nprint(f'Silhouette Score: {silhouette_avg:.2f}')\n\nSilhouette Score: 0.72\n\n\nSilhouette coefficient are calculated to evaluate the clustering model. As a measure of how similar an object is to its own cluster, a silhouette score of 0.72 in this study indicates a good cluster structure. The score suggests that each data point is much closer to the members of its own cluster than to the members of other clusters. Also, it has strong separation and cohesion and reliable clustering results. Also, it is important to consider other factors and domain-specific knowledge when evaluating the effective of a clustering model.\nIn summary, clustering results may be useful for understanding different groups of characteristics in penguin populations. For example, different clusters may represent different species of penguins, or penguins may be grouped according to their biometric characteristics (e.g., bill length, bill depth, flipper length, and weight). These clustering results can be used to further investigate the specific attributes of the penguins in each cluster, providing a basis for understanding the ecological and behavioral patterns of penguin populations.\n\n\nReference\n\nhttps://ocean.si.edu/ocean-life/seabirds/how-big-do-penguins-get\nhttps://www.worldwildlife.org/species/penguin\nhttps://www.kaggle.com/code/iisubbotina/clustering-penguins\nhttps://www.kaggle.com/datasets/youssefaboelwafa/clustering-penguins-species/data"
  },
  {
    "objectID": "posts/fetal health classification/index.html#step-2-label-encoding",
    "href": "posts/fetal health classification/index.html#step-2-label-encoding",
    "title": "Classifying Fetal Health Conditions Using Machine Learning Algorithms and Cardiotocography (CTG) Data",
    "section": "Step 2: Label encoding",
    "text": "Step 2: Label encoding\nThe dataset was split into 2 parts. 30% of the data will be reserved for testing, and 70% will be used for training. Then the “fit-transform” method is called on the scaler object to scale the training data. This method computes the minimum and maximum values to be used for later scaling(fit), and then scales the training data(transform).\n\n#Label encoding\n#Dara Splitting\ndf_features = df.drop(['fetal_health'], axis=1)\ndf_target = df['fetal_health']\n\n# split df at 70-30 ratio\nX_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size=0.3, random_state=123)\n\n# Initialize the MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Fit the scaler on the training data and transform the training data\nscaled_X_train = scaler.fit_transform(X_train)\n# Transform the test data using the fitted scaler\nscaled_X_test = scaler.transform(X_test)  # Only transform, no fitting\n\n#Convert the scaled training data back to a DataFrame\nscaled_X_train = pd.DataFrame(scaled_X_train, columns = X_train.columns, index=X_train.index)\nscaled_X_train.head()\n\n\n\n\n\n\n\n\nbaseline value\naccelerations\nfetal_movement\nuterine_contractions\nlight_decelerations\nsevere_decelerations\nprolongued_decelerations\nabnormal_short_term_variability\nmean_value_of_short_term_variability\npercentage_of_time_with_abnormal_long_term_variability\n...\nhistogram_width\nhistogram_min\nhistogram_max\nhistogram_number_of_peaks\nhistogram_number_of_zeroes\nhistogram_mode\nhistogram_mean\nhistogram_median\nhistogram_variance\nhistogram_tendency\n\n\n\n\n468\n0.703704\n0.000000\n0.004158\n0.133333\n0.000000\n0.0\n0.0\n0.960000\n0.014706\n0.373626\n...\n0.197740\n0.759259\n0.413793\n0.111111\n0.0\n0.666667\n0.642202\n0.620370\n0.000000\n0.0\n\n\n1143\n0.296296\n0.222222\n0.000000\n0.400000\n0.400000\n0.0\n0.0\n0.120000\n0.220588\n0.000000\n...\n0.372881\n0.222222\n0.181034\n0.222222\n0.0\n0.579365\n0.440367\n0.444444\n0.118959\n1.0\n\n\n162\n0.296296\n0.222222\n0.000000\n0.400000\n0.066667\n0.0\n0.0\n0.266667\n0.147059\n0.109890\n...\n0.440678\n0.157407\n0.224138\n0.111111\n0.0\n0.515873\n0.467890\n0.444444\n0.029740\n1.0\n\n\n919\n0.296296\n0.000000\n0.000000\n0.266667\n0.000000\n0.0\n0.0\n0.360000\n0.088235\n0.109890\n...\n0.112994\n0.518519\n0.060345\n0.055556\n0.0\n0.500000\n0.422018\n0.398148\n0.007435\n0.5\n\n\n1103\n0.296296\n0.388889\n0.000000\n0.200000\n0.000000\n0.0\n0.0\n0.093333\n0.308824\n0.000000\n...\n0.282486\n0.500000\n0.301724\n0.111111\n0.1\n0.515873\n0.495413\n0.462963\n0.029740\n0.5\n\n\n\n\n5 rows × 21 columns\n\n\n\nIn machine learning, we CANNOT “fit” the scaler to the test data because doing so would make the model indirectly aware of the specifics of the test data. If we fit on the test data, the scaler will adjust its parameters according to the range of the test data, which is equivalent to leaking information about the test data during the training phase of the model. This can lead to overfitting the model to the test data, which affects our purpose of realistically evaluating the generalization ability of the model.\nThe correct approach is to only fit on the training data and use this fitted scaler to transform the test data. This ensures that our model evaluation is fair because the test data is scaled to the range of the training data, not its own range. In short, this ensures that the data in the test phase is not previously seen by the model, thus providing a fair assessment of the model’s performance when dealing with new data. The test data fitted by scaler from training data is shown as below.\n\nscaled_X_test = pd.DataFrame(scaled_X_test, columns = X_test.columns, index=X_test.index)\nscaled_X_test.head()\n\n\n\n\n\n\n\n\nbaseline value\naccelerations\nfetal_movement\nuterine_contractions\nlight_decelerations\nsevere_decelerations\nprolongued_decelerations\nabnormal_short_term_variability\nmean_value_of_short_term_variability\npercentage_of_time_with_abnormal_long_term_variability\n...\nhistogram_width\nhistogram_min\nhistogram_max\nhistogram_number_of_peaks\nhistogram_number_of_zeroes\nhistogram_mode\nhistogram_mean\nhistogram_median\nhistogram_variance\nhistogram_tendency\n\n\n\n\n1542\n0.703704\n0.166667\n0.000000\n0.533333\n0.0\n0.0\n0.0\n0.480000\n0.073529\n0.065934\n...\n0.248588\n0.648148\n0.387931\n0.166667\n0.0\n0.698413\n0.697248\n0.666667\n0.011152\n0.5\n\n\n753\n0.444444\n0.000000\n0.002079\n0.000000\n0.0\n0.0\n0.0\n0.586667\n0.044118\n0.362637\n...\n0.090395\n0.694444\n0.189655\n0.111111\n0.0\n0.547619\n0.532110\n0.500000\n0.007435\n0.0\n\n\n454\n0.500000\n0.000000\n0.006237\n0.200000\n0.0\n0.0\n0.0\n0.613333\n0.044118\n0.186813\n...\n0.107345\n0.694444\n0.215517\n0.111111\n0.0\n0.603175\n0.577982\n0.546296\n0.003717\n0.5\n\n\n534\n0.574074\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.680000\n0.044118\n0.395604\n...\n0.525424\n0.194444\n0.387931\n0.166667\n0.0\n0.658730\n0.623853\n0.601852\n0.003717\n1.0\n\n\n599\n0.777778\n0.000000\n0.008316\n0.266667\n0.0\n0.0\n0.0\n0.840000\n0.058824\n0.000000\n...\n0.163842\n0.833333\n0.431034\n0.055556\n0.0\n0.714286\n0.706422\n0.675926\n0.000000\n0.0\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "posts/fetal health classification/index.html#step-3-modeling",
    "href": "posts/fetal health classification/index.html#step-3-modeling",
    "title": "Classifying Fetal Health Conditions Using Machine Learning Algorithms and Cardiotocography (CTG) Data",
    "section": "Step 3: Modeling",
    "text": "Step 3: Modeling\nIn this study, Machine Learning algorithms including Naive Bayes, Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors (KNN), and Support Vector Machines (SVM) are applied to train models.\nModel 1: Naive Bayes\n\n# Naive Bayes\nnaive_bayes = MultinomialNB()\n\n# Train the NB model on the train data\nnaive_bayes.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_nb = naive_bayes.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_nb = accuracy_score(y_test, y_pred_nb)\nprecision_nb = precision_score(y_test, y_pred_nb, average='weighted')\nrecall_nb = recall_score(y_test, y_pred_nb, average='weighted')\nf1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_nb, 3))\nprint(\"Precision:\", round(precision_nb, 3))\nprint(\"Recall:\", round(recall_nb, 3))\nprint(\"F1-Score:\", round(f1_nb, 3))\n\nAccuracy: 0.809\nPrecision: 0.792\nRecall: 0.809\nF1-Score: 0.75\n\n\nModel 2: Logistic Regression\n\n# Logistic regression \nlog_reg = LogisticRegression(random_state=123)\n\n# Train the LR model on the train data\nlog_reg.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_lr = log_reg.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nprecision_lr = precision_score(y_test, y_pred_lr, average='weighted')\nrecall_lr = recall_score(y_test, y_pred_lr, average='weighted')\nf1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_lr, 3))\nprint(\"Precision:\", round(precision_lr, 3))\nprint(\"Recall:\", round(recall_lr, 3))\nprint(\"F1-Score:\", round(f1_lr, 3))\n\nAccuracy: 0.901\nPrecision: 0.896\nRecall: 0.901\nF1-Score: 0.897\n\n\nModel 3: Decision Tree\n\n# Decision tree \nd_tree = DecisionTreeClassifier(random_state=123)\n\n# Train the DT model on the train data\nd_tree.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_dt = d_tree.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nprecision_dt = precision_score(y_test, y_pred_dt, average='weighted')\nrecall_dt = recall_score(y_test, y_pred_dt, average='weighted')\nf1_dt = f1_score(y_test, y_pred_dt, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_dt, 3))\nprint(\"Precision:\", round(precision_dt, 3))\nprint(\"Recall:\", round(recall_dt, 3))\nprint(\"F1-Score:\", round(f1_dt, 3))\n\nAccuracy: 0.92\nPrecision: 0.923\nRecall: 0.92\nF1-Score: 0.921\n\n\nModel 4: Random Forest\n\n# Random forest\nr_forest = RandomForestClassifier(random_state=123)\n\n# Train the DT model on the train data\nr_forest.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_rf = r_forest.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprecision_rf = precision_score(y_test, y_pred_rf, average='weighted')\nrecall_rf = recall_score(y_test, y_pred_rf, average='weighted')\nf1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_rf, 3))\nprint(\"Precision:\", round(precision_rf, 3))\nprint(\"Recall:\", round(recall_rf, 3))\nprint(\"F1-Score:\", round(f1_rf, 3))\n\nAccuracy: 0.946\nPrecision: 0.946\nRecall: 0.946\nF1-Score: 0.946\n\n\nModel 5: K-Nearest Neighbors (KNN)\n\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Ensure that scaled_X_train and scaled_X_test are C-ordered NumPy arrays\nscaled_X_train = np.ascontiguousarray(scaled_X_train)\nscaled_X_test = np.ascontiguousarray(scaled_X_test)\n\n# Initialization and training of KNN classifiers\nknn = KNeighborsClassifier()\nknn.fit(scaled_X_train, y_train)\n\n# Predictions on the test set\ny_pred_knn = knn.predict(scaled_X_test)\n\n# Calculation of evaluation indicators\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nprecision_knn = precision_score(y_test, y_pred_knn, average='weighted')\nrecall_knn = recall_score(y_test, y_pred_knn, average='weighted')\nf1_knn = f1_score(y_test, y_pred_knn, average='weighted')\n\n# 打印结果\nprint(\"Accuracy:\", round(accuracy_knn, 3))\nprint(\"Precision:\", round(precision_knn, 3))\nprint(\"Recall:\", round(recall_knn, 3))\nprint(\"F1-Score:\", round(f1_knn, 3))\n\nAccuracy: 0.907\nPrecision: 0.903\nRecall: 0.907\nF1-Score: 0.901\n\n\nModel 6: Support Vector Machines (SVM)\n\n# Support vector machines classifier \nsvm = SVC(random_state=123)\n\n# Train SVC on train data\nsvm.fit(scaled_X_train, y_train)\n\n# Predict on the test data\ny_pred_svm = svm.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nprecision_svm = precision_score(y_test, y_pred_svm, average='weighted')\nrecall_svm = recall_score(y_test, y_pred_svm, average='weighted')\nf1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_svm, 3))\nprint(\"Precision:\", round(precision_svm, 3))\nprint(\"Recall:\", round(recall_svm, 3))\nprint(\"F1-Score:\", round(f1_svm, 3))\n\nAccuracy: 0.913\nPrecision: 0.912\nRecall: 0.913\nF1-Score: 0.911"
  },
  {
    "objectID": "posts/fetal health classification/index.html#step-5-evaluation",
    "href": "posts/fetal health classification/index.html#step-5-evaluation",
    "title": "Classifying Fetal Health Conditions Using Machine Learning Algorithms and Cardiotocography (CTG) Data",
    "section": "Step 5: Evaluation",
    "text": "Step 5: Evaluation\nThe performance of each machine learning model, trained using various algorithms, is evaluated using metrics such as accuracy, precision, recall, and F1-Score. These metrics provide a comprehensive understanding of different facets of model performance, enabling the selection of the most effective model. Metrics have shown in belowing table. As a result, the model trained by random forest algorithm has the best performance among 6 models.\n\n\n\n\n\n\n\n\n\n\n\nModel No.\nML Algorithm\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\n1\nNaive Bayes\n0.809\n0.792\n0.809\n0.75\n\n\n2\nLogistic Regression\n0.901\n0.896\n0.901\n0.897\n\n\n3\nDecision Tree\n0.92\n0.923\n0.92\n0.921\n\n\n4\nRandom Forest\n0.946\n0.946\n0.946\n0.946\n\n\n5\nKNN\n0.907\n0.903\n0.907\n0.901\n\n\n6\nSVM\n0.913\n0.912\n0.913\n0.911"
  },
  {
    "objectID": "posts/Predicting humidity with Support Vector Regression/index.html",
    "href": "posts/Predicting humidity with Support Vector Regression/index.html",
    "title": "Predicting Humidity Using Linear and Non-linear Regression",
    "section": "",
    "text": "Temperature and humidity are critical for weather forecasting and climate modeling. They affect agricultural production, building design, energy use etc. [1] In the area of environmental science, understanding the relationship between temperature and humidity is critical in detecting changes in ecosystems, predicting weather patterns, and even assessing global climate change. Moreover, temperature and humidity based analysis can help develop more effective strategies and decisions for industries that need to consider environmental factors. Therefore, the prediction of climate information is a topic of interests that has been analyzed through a long period. Machine Learning, a powerful tool in dealing large datasets, is expected to provide a more precise climate prediction than ever before.\nIn this blog, we explore the application of machine learning algorithms for linear and non-linear regression, focusing on predicting humidity based on average daily temperature. We will employ the Lasso algorithm for linear regression and leverage Support Vector Regression(SVR) for the non-linear regression analysis. Through a comparative analysis of linear and non-linear regression results, this blog illustrates how machine learning algorithms can be effectively utilized in climate prediction scenarios."
  },
  {
    "objectID": "posts/Predicting humidity with Support Vector Regression/index.html#evaluation-of-lasso-regression-analysis",
    "href": "posts/Predicting humidity with Support Vector Regression/index.html#evaluation-of-lasso-regression-analysis",
    "title": "Predicting Humidity Using Linear and Non-linear Regression",
    "section": "Evaluation of Lasso Regression Analysis",
    "text": "Evaluation of Lasso Regression Analysis\n\n# LASSO: Plot scatter and regression diagram\nplt.figure(figsize=(10, 6))\nplt.scatter(train_data['meantemp'], train_data['humidity'], color='blue', alpha=0.5, label='Actual humidity (train data)')\nplt.plot(train_data['meantemp'], y_train_pred_lasso, color='red', label='Estimated humidity(train data)')\nplt.scatter(test_data['meantemp'], test_data['humidity'], color='green', alpha=0.5, label='Actual humidity (test data)')\nplt.plot(test_data['meantemp'], y_test_pred_lasso, color='orange', label='Estimated humidity(test data)')\nplt.xlabel('Daily Average Temperature (°C)')\nplt.ylabel('Humidity(%)')\nplt.title('Daily Average Temperatrue vs Humidity(Actual vs Estimated)')\nplt.legend()\nplt.show()\n\n\n\n\nMean Squared Error is a measure of the average of the squares of the errors or deviations. In simpler terms, it calculates the average squared difference between the estimated values and the actual value. The RMSE values indicate the average magnitude of the errors in predicting humidity from temperature, thus a lower RMSE value is better.\n\\[ RMSE =\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2} \\]\nRMSE = Root mean squared error\nn = number of data points\n\\(Y_i\\)=Observed values\n\\(\\hat{Y}_i\\) = Predicted values\nThe Coefficient of Determination, often denoted as R², is a statistical measure that represents the proportion of variance for the dependent variable that’s explained by the independent variables in a regression model.\n\\[ R^2 = 1-\\frac{RSS}{TSS} \\]\n\\(R^2\\) = coefficient of determination\nRSS= sum of squares of residuals. It is the sum of the squared differences between actual and predicted values.\nTSS = total sum of squares. It is the sum of the squared differences from the mean of the actual values.\nIn this study, the RMSE of 13.75 on the training data and 15.79 on the testing data suggests moderate errors in the model’s predictions. The higher RMSE on the testing data compared to the training data may indicate a slight overfitting to the training data or a difference in the distribution of the training and testing sets.\nIn lasso regression model. the R² values in both the training (0.33) and testing (0.31) datasets are relatively low. This indicates that 33% of the variance in humidity is explained by temperature in the training set, and about 31% in the testing set. These indexes suggest that temperature alone may not be sufficient to accurately predict humidity, or that the relationship between temperature and humidity is not strongly linear.\nIn conclusion, the model’s performance is moderate. More relevant features are expected to apply in the model to make a better prediction of humidity."
  },
  {
    "objectID": "posts/Predicting humidity with Support Vector Regression/index.html#results-evaluation-of-svr-model",
    "href": "posts/Predicting humidity with Support Vector Regression/index.html#results-evaluation-of-svr-model",
    "title": "Predicting Humidity Using Linear and Non-linear Regression",
    "section": "Results Evaluation of SVR Model",
    "text": "Results Evaluation of SVR Model\n\n# SVR: Plot scatter and regression diagram\nplt.figure(figsize=(10, 6))\nplt.scatter(train_data['meantemp'], train_data['humidity'], color='blue', alpha=0.5, label='Actual humidity (train data)')\nplt.plot(train_data['meantemp'], y_train_pred_svr, color='red', label='Estimated humidity(train data)')\nplt.scatter(test_data['meantemp'], test_data['humidity'], color='green', alpha=0.5, label='Actual humidity (test data)')\nplt.plot(test_data['meantemp'], y_test_pred_svr, color='orange', label='Estimated humidity(test data)')\nplt.xlabel('Daily Average Temperature (°C)')\nplt.ylabel('Humidity(%)')\nplt.title('Daily Average Temperatrue vs Humidity(Actual vs Estimated)')\nplt.legend()\nplt.show()\n\n\n\n\nThe RMSE for the training data is 12.99, which is slightly better than the RMSE for the Lasso model. The RMSE for the testing data is 17.37, which is higher than that of the training data and also higher compared to the Lasso model. This indicates that the model may not generalize as well to new, unseen data.\nThe R² value on the training data is 0.40, suggesting the model explains 40% of the variance in humidity based on temperature, which is an improvement over the Lasso model. The R² value on the testing data is 0.16, much lower than the training data, indicating a significant drop in model performance on unseen data."
  }
]