{
  "hash": "7c589fbb784673d3234c62c7eff3f929",
  "result": {
    "markdown": "---\ntitle: \"Classifying Fetal Health Conditions Using Machine Learning Algorithms and Cardiotocography (CTG) Data\"\nauthor: \"Xiaoying Yang\"\ndate: \"2023-11-26\"\ncategories: [ code, analysis]\nimage: \"fetal health classification.jpg\"\n---\n\n![](fetal%20health%20classification.jpg)\n\n# Cardiotocograms (CTGs) and fetal health\n\nThe reduction of child mortality, a central objective in several of the United Nations' Sustainable Development Goals, stands as a crucial measure of human progress. In this context, Cardiotocograms (CTGs) emerge as an effective and economically viable method to monitor fetal health. They equip healthcare professionals with vital data, enabling timely interventions to prevent both child and maternal mortality. Functioning through the emission and reception of ultrasound pulses, CTGs provide critical insights into fetal heart rate (FHR), fetal movements, uterine contractions, and other significant parameters, thereby playing a pivotal role in prenatal care.\n\nIn clinical environments, specialized procedures exist to evaluate whether a patient's Cardiotocogram (CTG) is normal, a process that necessitates expert domain knowledge. As machine learning technology advances, there is growing potential to develop sophisticated models that can classify fetal health more efficiently. These models would be based on features extracted from CTG outputs, aiming to enhance the prediction of child and maternal mortality. This integration of machine learning into fetal health assessment represents a significant stride towards more accurate and accessible prenatal care.\n\n# Dataset and Model description\n\nIn this blog, we explore the use of a Kaggle dataset to classify fetal health conditions using machine learning models. This dataset comprises 2126 instances of features derived from Cardiotocogram (CTG) exams. These features have been categorized into three classes by expert obstetricians: Normal, Suspect, and Pathological. Our objective is to develop a multiclass model capable of accurately classifying CTG features into these three distinct fetal health states, demonstrating the potential of machine learning in enhancing prenatal care diagnostics.\n\nHere are steps of implementing the machine learning models to do the classification:\n\n1.  Load & Pre-process data: Handling missing values, outliers, etc.\n\n2.  Label encoding: Split data into training dataset and test dataset, and transform data by scaling and normalization, which makes continuous variables into a common scale to ease the comparison between variables with various units and ranges.\n\n3.  Modeling\n\n4.  Evaluation\n\n    ## Step 1: Load & Pre-process data\n\n    The histogram below provides a visual overview of how the fetal health cases are distributed across these three categories, indicating the relative frequencies of normal, suspect, and pathological fetal health conditions in the dataset.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# import library\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n# load & preprocess data\ndf = pd.read_csv('https://raw.githubusercontent.com/xiaoyingyang96/xiaoyingyang96.github.io/main/Data/fetal_health.csv')\n\n#handle missing values, drop duplicated row\ndf.isnull().sum()\ndf = df.drop_duplicates()\n\n# Plotting the histogram for the distribution of fetal_health\nplt.figure(figsize=(8, 6))\nplt.hist(df['fetal_health'], bins=np.arange(1, 5) - 0.5, rwidth=0.8)\nplt.xticks(ticks=[1, 2, 3], labels=['Normal', 'Suspect', 'Pathological'])\nplt.xlabel('Fetal Health Status')\nplt.ylabel('Frequency')\nplt.title('Distribution of Fetal Health')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=676 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Handling outlier for numeric attributes \n# Create a 2x3 grid of subplots\nfig, axes = plt.subplots(6, 4, figsize=(10, 12))\naxes = axes.flatten()\n    \n# Create a boxplot for each continuous variable\nfor i, column in enumerate(df.columns):\n    sns.boxplot(data=df, x=column, ax=axes[i])\n    axes[i].set_title(f\"{column}\", fontsize=10)\n    axes[i].set_xlabel(\"\")  # Remove x-axis title\n\n# Remove empty subplot\nfor i in range(len(df.columns), len(axes)):\n    fig.delaxes(axes[i])\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=966 height=1140}\n:::\n:::\n\n\nBoxplots are useful for indicating whether a distribution is skewed and whether there are potential unusual observations (outliers) in the dataset. In this example we applied boxplots to explore outliers from each data and diagrams for each attribute are presented above. The boxplots reveal outliers in several attributes. Nonetheless, considering that the dataset has been reviewed and interpreted by clinic experts, the apparent outliers identified in the boxplots might not be anomalous in a clinical setting. Rather, they may signify clinically relevant variations, such as indicators of fetal distress or maternal health conditions. No action will be taken on these values that out of boxplot range.\n\n## Step 2: Label encoding\n\nThe dataset was split into 2 parts. 30% of the data will be reserved for testing, and 70% will be used for training. Then the \"fit-transform\" method is called on the scaler object to scale the training data. This method computes the minimum and maximum values to be used for later scaling(fit), and then scales the training data(transform).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n#Label encoding\n#Dara Splitting\ndf_features = df.drop(['fetal_health'], axis=1)\ndf_target = df['fetal_health']\n\n# split df at 70-30 ratio\nX_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size=0.3, random_state=123)\n\n# Initialize the MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Fit the scaler on the training data and transform the training data\nscaled_X_train = scaler.fit_transform(X_train)\n# Transform the test data using the fitted scaler\nscaled_X_test = scaler.transform(X_test)  # Only transform, no fitting\n\n#Convert the scaled training data back to a DataFrame\nscaled_X_train = pd.DataFrame(scaled_X_train, columns = X_train.columns, index=X_train.index)\nscaled_X_train.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseline value</th>\n      <th>accelerations</th>\n      <th>fetal_movement</th>\n      <th>uterine_contractions</th>\n      <th>light_decelerations</th>\n      <th>severe_decelerations</th>\n      <th>prolongued_decelerations</th>\n      <th>abnormal_short_term_variability</th>\n      <th>mean_value_of_short_term_variability</th>\n      <th>percentage_of_time_with_abnormal_long_term_variability</th>\n      <th>...</th>\n      <th>histogram_width</th>\n      <th>histogram_min</th>\n      <th>histogram_max</th>\n      <th>histogram_number_of_peaks</th>\n      <th>histogram_number_of_zeroes</th>\n      <th>histogram_mode</th>\n      <th>histogram_mean</th>\n      <th>histogram_median</th>\n      <th>histogram_variance</th>\n      <th>histogram_tendency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>468</th>\n      <td>0.703704</td>\n      <td>0.000000</td>\n      <td>0.004158</td>\n      <td>0.133333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.960000</td>\n      <td>0.014706</td>\n      <td>0.373626</td>\n      <td>...</td>\n      <td>0.197740</td>\n      <td>0.759259</td>\n      <td>0.413793</td>\n      <td>0.111111</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.642202</td>\n      <td>0.620370</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1143</th>\n      <td>0.296296</td>\n      <td>0.222222</td>\n      <td>0.000000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.120000</td>\n      <td>0.220588</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.372881</td>\n      <td>0.222222</td>\n      <td>0.181034</td>\n      <td>0.222222</td>\n      <td>0.0</td>\n      <td>0.579365</td>\n      <td>0.440367</td>\n      <td>0.444444</td>\n      <td>0.118959</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>0.296296</td>\n      <td>0.222222</td>\n      <td>0.000000</td>\n      <td>0.400000</td>\n      <td>0.066667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.266667</td>\n      <td>0.147059</td>\n      <td>0.109890</td>\n      <td>...</td>\n      <td>0.440678</td>\n      <td>0.157407</td>\n      <td>0.224138</td>\n      <td>0.111111</td>\n      <td>0.0</td>\n      <td>0.515873</td>\n      <td>0.467890</td>\n      <td>0.444444</td>\n      <td>0.029740</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td>0.296296</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.266667</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.360000</td>\n      <td>0.088235</td>\n      <td>0.109890</td>\n      <td>...</td>\n      <td>0.112994</td>\n      <td>0.518519</td>\n      <td>0.060345</td>\n      <td>0.055556</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.422018</td>\n      <td>0.398148</td>\n      <td>0.007435</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1103</th>\n      <td>0.296296</td>\n      <td>0.388889</td>\n      <td>0.000000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.093333</td>\n      <td>0.308824</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.282486</td>\n      <td>0.500000</td>\n      <td>0.301724</td>\n      <td>0.111111</td>\n      <td>0.1</td>\n      <td>0.515873</td>\n      <td>0.495413</td>\n      <td>0.462963</td>\n      <td>0.029740</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>\n```\n:::\n:::\n\n\nIn machine learning, we **CANNOT** \"fit\" the scaler to the test data because doing so would make the model indirectly aware of the specifics of the test data. If we fit on the test data, the scaler will adjust its parameters according to the range of the test data, which is equivalent to leaking information about the test data during the training phase of the model. This can lead to overfitting the model to the test data, which affects our purpose of realistically evaluating the generalization ability of the model.\\\nThe correct approach is to only fit on the training data and use this fitted scaler to transform the test data. This ensures that our model evaluation is fair because the test data is scaled to the range of the training data, not its own range. In short, this ensures that the data in the test phase is not previously seen by the model, thus providing a fair assessment of the model's performance when dealing with new data. The test data fitted by scaler from training data is shown as below.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nscaled_X_test = pd.DataFrame(scaled_X_test, columns = X_test.columns, index=X_test.index)\nscaled_X_test.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseline value</th>\n      <th>accelerations</th>\n      <th>fetal_movement</th>\n      <th>uterine_contractions</th>\n      <th>light_decelerations</th>\n      <th>severe_decelerations</th>\n      <th>prolongued_decelerations</th>\n      <th>abnormal_short_term_variability</th>\n      <th>mean_value_of_short_term_variability</th>\n      <th>percentage_of_time_with_abnormal_long_term_variability</th>\n      <th>...</th>\n      <th>histogram_width</th>\n      <th>histogram_min</th>\n      <th>histogram_max</th>\n      <th>histogram_number_of_peaks</th>\n      <th>histogram_number_of_zeroes</th>\n      <th>histogram_mode</th>\n      <th>histogram_mean</th>\n      <th>histogram_median</th>\n      <th>histogram_variance</th>\n      <th>histogram_tendency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1542</th>\n      <td>0.703704</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.533333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.480000</td>\n      <td>0.073529</td>\n      <td>0.065934</td>\n      <td>...</td>\n      <td>0.248588</td>\n      <td>0.648148</td>\n      <td>0.387931</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.698413</td>\n      <td>0.697248</td>\n      <td>0.666667</td>\n      <td>0.011152</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>0.444444</td>\n      <td>0.000000</td>\n      <td>0.002079</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.586667</td>\n      <td>0.044118</td>\n      <td>0.362637</td>\n      <td>...</td>\n      <td>0.090395</td>\n      <td>0.694444</td>\n      <td>0.189655</td>\n      <td>0.111111</td>\n      <td>0.0</td>\n      <td>0.547619</td>\n      <td>0.532110</td>\n      <td>0.500000</td>\n      <td>0.007435</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.006237</td>\n      <td>0.200000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.613333</td>\n      <td>0.044118</td>\n      <td>0.186813</td>\n      <td>...</td>\n      <td>0.107345</td>\n      <td>0.694444</td>\n      <td>0.215517</td>\n      <td>0.111111</td>\n      <td>0.0</td>\n      <td>0.603175</td>\n      <td>0.577982</td>\n      <td>0.546296</td>\n      <td>0.003717</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>534</th>\n      <td>0.574074</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.680000</td>\n      <td>0.044118</td>\n      <td>0.395604</td>\n      <td>...</td>\n      <td>0.525424</td>\n      <td>0.194444</td>\n      <td>0.387931</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.658730</td>\n      <td>0.623853</td>\n      <td>0.601852</td>\n      <td>0.003717</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>599</th>\n      <td>0.777778</td>\n      <td>0.000000</td>\n      <td>0.008316</td>\n      <td>0.266667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.840000</td>\n      <td>0.058824</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.163842</td>\n      <td>0.833333</td>\n      <td>0.431034</td>\n      <td>0.055556</td>\n      <td>0.0</td>\n      <td>0.714286</td>\n      <td>0.706422</td>\n      <td>0.675926</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Step 3: Modeling\n\nIn this study, Machine Learning algorithms including Naive Bayes, Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors (KNN), and Support Vector Machines (SVM) are applied to train models.\n\nModel 1: Naive Bayes\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Naive Bayes\nnaive_bayes = MultinomialNB()\n\n# Train the NB model on the train data\nnaive_bayes.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_nb = naive_bayes.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_nb = accuracy_score(y_test, y_pred_nb)\nprecision_nb = precision_score(y_test, y_pred_nb, average='weighted')\nrecall_nb = recall_score(y_test, y_pred_nb, average='weighted')\nf1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_nb, 3))\nprint(\"Precision:\", round(precision_nb, 3))\nprint(\"Recall:\", round(recall_nb, 3))\nprint(\"F1-Score:\", round(f1_nb, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.809\nPrecision: 0.792\nRecall: 0.809\nF1-Score: 0.75\n```\n:::\n:::\n\n\nModel 2: Logistic Regression\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Logistic regression \nlog_reg = LogisticRegression(random_state=123)\n\n# Train the LR model on the train data\nlog_reg.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_lr = log_reg.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nprecision_lr = precision_score(y_test, y_pred_lr, average='weighted')\nrecall_lr = recall_score(y_test, y_pred_lr, average='weighted')\nf1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_lr, 3))\nprint(\"Precision:\", round(precision_lr, 3))\nprint(\"Recall:\", round(recall_lr, 3))\nprint(\"F1-Score:\", round(f1_lr, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.901\nPrecision: 0.896\nRecall: 0.901\nF1-Score: 0.897\n```\n:::\n:::\n\n\nModel 3: Decision Tree\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Decision tree \nd_tree = DecisionTreeClassifier(random_state=123)\n\n# Train the DT model on the train data\nd_tree.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_dt = d_tree.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nprecision_dt = precision_score(y_test, y_pred_dt, average='weighted')\nrecall_dt = recall_score(y_test, y_pred_dt, average='weighted')\nf1_dt = f1_score(y_test, y_pred_dt, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_dt, 3))\nprint(\"Precision:\", round(precision_dt, 3))\nprint(\"Recall:\", round(recall_dt, 3))\nprint(\"F1-Score:\", round(f1_dt, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.92\nPrecision: 0.923\nRecall: 0.92\nF1-Score: 0.921\n```\n:::\n:::\n\n\nModel 4: Random Forest\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Random forest\nr_forest = RandomForestClassifier(random_state=123)\n\n# Train the DT model on the train data\nr_forest.fit(scaled_X_train, y_train)\n\n# Predict on test data\ny_pred_rf = r_forest.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprecision_rf = precision_score(y_test, y_pred_rf, average='weighted')\nrecall_rf = recall_score(y_test, y_pred_rf, average='weighted')\nf1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_rf, 3))\nprint(\"Precision:\", round(precision_rf, 3))\nprint(\"Recall:\", round(recall_rf, 3))\nprint(\"F1-Score:\", round(f1_rf, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.946\nPrecision: 0.946\nRecall: 0.946\nF1-Score: 0.946\n```\n:::\n:::\n\n\nModel 5: K-Nearest Neighbors (KNN)\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Ensure that scaled_X_train and scaled_X_test are C-ordered NumPy arrays\nscaled_X_train = np.ascontiguousarray(scaled_X_train)\nscaled_X_test = np.ascontiguousarray(scaled_X_test)\n\n# Initialization and training of KNN classifiers\nknn = KNeighborsClassifier()\nknn.fit(scaled_X_train, y_train)\n\n# Predictions on the test set\ny_pred_knn = knn.predict(scaled_X_test)\n\n# Calculation of evaluation indicators\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nprecision_knn = precision_score(y_test, y_pred_knn, average='weighted')\nrecall_knn = recall_score(y_test, y_pred_knn, average='weighted')\nf1_knn = f1_score(y_test, y_pred_knn, average='weighted')\n\n# 打印结果\nprint(\"Accuracy:\", round(accuracy_knn, 3))\nprint(\"Precision:\", round(precision_knn, 3))\nprint(\"Recall:\", round(recall_knn, 3))\nprint(\"F1-Score:\", round(f1_knn, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.907\nPrecision: 0.903\nRecall: 0.907\nF1-Score: 0.901\n```\n:::\n:::\n\n\nModel 6: Support Vector Machines (SVM)\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Support vector machines classifier \nsvm = SVC(random_state=123)\n\n# Train SVC on train data\nsvm.fit(scaled_X_train, y_train)\n\n# Predict on the test data\ny_pred_svm = svm.predict(scaled_X_test)\n\n# Calculate evaluation metrics\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nprecision_svm = precision_score(y_test, y_pred_svm, average='weighted')\nrecall_svm = recall_score(y_test, y_pred_svm, average='weighted')\nf1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n\n# Print results\nprint(\"Accuracy:\", round(accuracy_svm, 3))\nprint(\"Precision:\", round(precision_svm, 3))\nprint(\"Recall:\", round(recall_svm, 3))\nprint(\"F1-Score:\", round(f1_svm, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.913\nPrecision: 0.912\nRecall: 0.913\nF1-Score: 0.911\n```\n:::\n:::\n\n\n## Step 5: Evaluation\n\nThe performance of each machine learning model, trained using various algorithms, is evaluated using metrics such as accuracy, precision, recall, and F1-Score. These metrics provide a comprehensive understanding of different facets of model performance, enabling the selection of the most effective model. Metrics have shown in belowing table. As a result, the model trained by random forest algorithm has the best performance among 6 models.\n\n| Model No. | ML Algorithm        | Accuracy | Precision | Recall | F1-Score |\n|-----------|---------------------|----------|-----------|--------|----------|\n| 1         | Naive Bayes         | 0.809    | 0.792     | 0.809  | 0.75     |\n| 2         | Logistic Regression | 0.901    | 0.896     | 0.901  | 0.897    |\n| 3         | Decision Tree       | 0.92     | 0.923     | 0.92   | 0.921    |\n| 4         | Random Forest       | 0.946    | 0.946     | 0.946  | 0.946    |\n| 5         | KNN                 | 0.907    | 0.903     | 0.907  | 0.901    |\n| 6         | SVM                 | 0.913    | 0.912     | 0.913  | 0.911    |\n\n# Reference\n\n1.  https://www.osmosis.org/learn/Assessment_During_Pregnancy\n2.  https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification/data\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}